{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Solution for Siraj's defi \n",
    "\n",
    "My solution is generic and works with any number of layers, and any number of nodes in each layer.\n",
    "\n",
    "Take care of the difference of numpy dot and * !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot, matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[[0 1]\n",
      " [1 0]]\n",
      "B\n",
      "[[0]\n",
      " [1]]\n",
      "C\n",
      "[[3 1]\n",
      " [2 1]]\n",
      "A * B\n",
      "[[0 0]\n",
      " [1 0]]\n",
      "A.dot(B)\n",
      "[[1]\n",
      " [0]]\n",
      "The solution is different because * is the Element-wise Multiplication and dot is matrice multiplication\n",
      "A * C\n",
      "[[0 1]\n",
      " [2 0]]\n",
      "A.dot(C)\n",
      "[[2 1]\n",
      " [3 1]]\n",
      "np.matmul(A, C)\n",
      "[[2 1]\n",
      " [3 1]]\n",
      "The result of dot and matmul are the same if and only if the matrices are two dimensional\n"
     ]
    }
   ],
   "source": [
    "A = array([[0, 1], [1, 0]])\n",
    "B = array([[0, 1]]).T\n",
    "C = array([[3, 1], [2, 1]])\n",
    "print \"A\"\n",
    "print A\n",
    "print \"B\"\n",
    "print B\n",
    "print \"C\"\n",
    "print C\n",
    "print \"A * B\"\n",
    "print A * B\n",
    "print \"A.dot(B)\"\n",
    "print A.dot(B)\n",
    "print \"The solution is different because * is the Element-wise Multiplication and dot is matrice multiplication\"\n",
    "print \"A * C\"\n",
    "print A * C\n",
    "print \"A.dot(C)\"\n",
    "print A.dot(C)\n",
    "print \"np.matmul(A, C)\"\n",
    "print matmul(A, C)\n",
    "print \"The result of dot and matmul are the same if and only if the matrices are two dimensional\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layer_delta = list contenant les delta (layer_n, layer_n-1, ... , layer0)\n",
    "num_layer = variable (n-1 layer to 0)\n",
    "layer_weights = liste contenant (wo, w1, w2, ... wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    # Add variable for define the layers. Each layer is a tuple (nb_inputs, nb_neurons)\n",
    "    def __init__(self, *layers):\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # every time the program runs.\n",
    "        random.seed(1)\n",
    "\n",
    "        # We parse the array of tuples and initializes the weigths matrix for each layers\n",
    "        self.layer_weights = []\n",
    "        for layer in layers:\n",
    "            self.layer_weights.append(2 * random.random((layer[0], layer[1])) - 1)\n",
    "            \n",
    "        self.nb_layer = len(layers)\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def printLog(self, num_layer, layer_error, delta, layer_adjustment):\n",
    "        print \"Layer \", num_layer ,\"error\"\n",
    "        print layer_error\n",
    "        print \"Layer \", num_layer ,\"delta\"\n",
    "        print delta\n",
    "        print \"layer \", num_layer ,\"adjustment\"\n",
    "        print layer_adjustment\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in xrange(number_of_training_iterations):\n",
    "            \n",
    "            print \"====== Forward Propagation ======\"\n",
    "            outputs = self.think(training_set_inputs)\n",
    "\n",
    "            print \"====== Backward Propagation ======\"\n",
    "            \n",
    "            layer_delta = list()\n",
    "            layer_adjustments = list()\n",
    "            \n",
    "            # Calculate the error from the last layer which is based on the output of the Forward Propagation\n",
    "            # (The difference between the desired output and the predicted output).\n",
    "            layer_error = training_set_outputs - outputs[-1]\n",
    "\n",
    "            # Multiply the error by the gradient of the Sigmoid curve of the last \n",
    "            # layer output (output computed by the FP)\n",
    "            delta = layer_error * self.__sigmoid_derivative(outputs[-1])\n",
    "            layer_delta.append(delta)\n",
    "\n",
    "            # Compute the adjustment : multiply the (layer -1) by the delta\n",
    "            layer_adjustment = outputs[-2].T.dot(delta)\n",
    "            layer_adjustments.append(layer_adjustment)\n",
    "            \n",
    "            self.printLog(self.nb_layer, layer_error, delta, layer_adjustment)\n",
    "\n",
    "            for index in range(self.nb_layer -1):  # 0 to n-1 (n = output_layer already processed)\n",
    "\n",
    "                num_layer = (self.nb_layer - index) -1\n",
    "            \n",
    "                last_delta = layer_delta[-1]\n",
    "\n",
    "                # Calculate the error : delta error next layer * weight next layer\n",
    "                # layer1_error = layer2_delta.dot(self.layer2.synaptic_weights.T)\n",
    "                layer_error = last_delta.dot(self.layer_weights[num_layer].T)\n",
    "\n",
    "                # Compute the delta backprop error term : error * derivate of the next layer\n",
    "                # layer1_delta = layer1_error * self.__sigmoid_derivative(output_from_layer_1)\n",
    "                delta = layer_error * self.__sigmoid_derivative(outputs[num_layer-1])\n",
    "                layer_delta.append(delta)\n",
    "\n",
    "                # Compute the adjustment\n",
    "                # layer1_adjustment = training_set_inputs.T.dot(layer1_delta)\n",
    "                if (num_layer >= 2) :\n",
    "                    layer_adjustment = outputs[num_layer-2].T.dot(delta)\n",
    "                else :\n",
    "                    layer_adjustment = training_set_inputs.T.dot(delta)\n",
    "                layer_adjustments.append(layer_adjustment)\n",
    "                \n",
    "                self.printLog(num_layer, layer_error, delta, layer_adjustment)\n",
    "\n",
    "            # Adjust the weights.\n",
    "            # The two lists are stored in reverse order\n",
    "            for j in range(self.nb_layer):\n",
    "                self.layer_weights[j] += layer_adjustments[-(j+1)]\n",
    "\n",
    "    # The neural network thinks.\n",
    "    # The first layer is computing with the inputs so it's why the calcul is not in the loop\n",
    "    def think(self, inputs):\n",
    "        \n",
    "        # This list stores all the computes for each layer\n",
    "        layer_back_prop_value = list()\n",
    "        \n",
    "        # Compute the first layer and initialize the variable layer_back_prop_value\n",
    "        layer_back_prop_value.append(self.__sigmoid(dot(inputs, self.layer_weights[0])))\n",
    "        \n",
    "        print \"Layer 0\"\n",
    "        print layer_back_prop_value[-1]\n",
    "            \n",
    "        # Compute each layer using the value of the previous layer\n",
    "        for index in range(1,self.nb_layer):\n",
    "            back_prop_value = self.__sigmoid(dot(layer_back_prop_value[-1], self.layer_weights[index]))\n",
    "            layer_back_prop_value.append(back_prop_value)\n",
    "            \n",
    "            print \"Layer \", index\n",
    "            print back_prop_value\n",
    "\n",
    "        print \" \"\n",
    "        return layer_back_prop_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Random starting Layer weights: ======\n",
      "Layer  0\n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "Layer  1\n",
      "[[-0.5910955   0.75623487 -0.94522481  0.34093502]\n",
      " [-0.1653904   0.11737966 -0.71922612 -0.60379702]\n",
      " [ 0.60148914  0.93652315 -0.37315164  0.38464523]\n",
      " [ 0.7527783   0.78921333 -0.82991158 -0.92189043]]\n",
      "Layer  2\n",
      "[[-0.66033916]\n",
      " [ 0.75628501]\n",
      " [-0.80330633]\n",
      " [-0.15778475]]\n",
      " \n",
      "====== Forward Propagation ======\n",
      "Layer 0\n",
      "[[ 0.44856632  0.51939863  0.45968497  0.59156505]\n",
      " [ 0.25371248  0.42628115  0.14321233  0.41732254]\n",
      " [ 0.40795614  0.62674606  0.23841622  0.49377636]\n",
      " [ 0.28639589  0.32350963  0.31236398  0.51538526]]\n",
      "Layer  1\n",
      "[[ 0.59164352  0.78542488  0.18846121  0.37069838]\n",
      " [ 0.54483448  0.66937976  0.27965197  0.3774167 ]\n",
      " [ 0.5424749   0.73008524  0.20831066  0.35369355]\n",
      " [ 0.58735983  0.72188698  0.25967308  0.38872586]]\n",
      "Layer  2\n",
      "[[ 0.49835927]\n",
      " [ 0.46562171]\n",
      " [ 0.49269811]\n",
      " [ 0.47206999]]\n",
      " \n",
      "====== Backward Propagation ======\n",
      "Layer  3 error\n",
      "[[-0.49835927]\n",
      " [ 0.53437829]\n",
      " [ 0.50730189]\n",
      " [-0.47206999]]\n",
      "Layer  3 delta\n",
      "[[-0.12458847]\n",
      " [ 0.13296301]\n",
      " [ 0.12679842]\n",
      " [-0.11764924]]\n",
      "layer  3 adjustment\n",
      "[[-0.00158661]\n",
      " [-0.00120794]\n",
      " [ 0.00956639]\n",
      " [ 0.0031122 ]]\n",
      "Layer  2 error\n",
      "[[ 0.08227065 -0.0942244   0.10008271  0.01965816]\n",
      " [-0.08780068  0.10055793 -0.10681003 -0.02097954]\n",
      " [-0.08372996  0.09589575 -0.10185798 -0.02000686]\n",
      " [ 0.0776884  -0.08897636  0.09450838  0.01856326]]\n",
      "Layer  2 delta\n",
      "[[ 0.01987671 -0.01587989  0.01530701  0.00458588]\n",
      " [-0.02177368  0.02225453 -0.02151653 -0.00492963]\n",
      " [-0.02078143  0.01889729 -0.01679815 -0.00457346]\n",
      " [ 0.0188292  -0.01786344  0.01816857  0.00441097]]\n",
      "layer  2 adjustment\n",
      "[[  3.06461248e-04   1.11631849e-03  -2.42306670e-04   2.03873870e-04]\n",
      " [ -5.89102613e-03   7.30350085e-03  -5.87211642e-03  -1.15891642e-03]\n",
      " [  6.94569937e-03  -5.18709773e-03   5.62522628e-03   1.68951541e-03]\n",
      " [  2.11463298e-03   1.78101409e-05   1.14504465e-03   6.70680414e-04]]\n",
      "Layer  1 error\n",
      "[[-0.03666303 -0.01892953 -0.00686416 -0.014501  ]\n",
      " [ 0.04835725  0.02466513  0.01387802  0.02357421]\n",
      " [ 0.04089337  0.02049831  0.00970704  0.01742738]\n",
      " [-0.04030835 -0.02094161 -0.01048694 -0.01906859]]\n",
      "Layer  1 delta\n",
      "[[-0.00906877 -0.00472526 -0.00170488 -0.00350367]\n",
      " [ 0.00915608  0.00603224  0.00170287  0.00573241]\n",
      " [ 0.00987689  0.00479528  0.00176255  0.00435617]\n",
      " [-0.00823795 -0.0045831  -0.00225252 -0.00476263]]\n",
      "layer  1 adjustment\n",
      "[[ 0.01903297  0.01082752  0.00346541  0.01008858]\n",
      " [ 0.00091813  0.00144915 -0.00054965  0.00096978]\n",
      " [ 0.00172625  0.00151917 -0.00049199  0.00182228]]\n",
      "====== New weights after training: ======\n",
      "Layer  1\n",
      "[[-0.14692302  0.45147651 -0.99630584 -0.38524627]\n",
      " [-0.70557009 -0.81387366 -0.62802923 -0.30790877]\n",
      " [-0.2047388   0.07915264 -0.16210296  0.37226128]]\n",
      "Layer  2\n",
      "[[-0.59078904  0.75735119 -0.94546712  0.34113889]\n",
      " [-0.17128142  0.12468316 -0.72509824 -0.60495594]\n",
      " [ 0.60843484  0.93133605 -0.36752642  0.38633475]\n",
      " [ 0.75489294  0.78923114 -0.82876653 -0.92121975]]\n",
      "Layer  3\n",
      "[[-0.66192577]\n",
      " [ 0.75507706]\n",
      " [-0.79373994]\n",
      " [-0.15467255]]\n",
      " \n",
      "Considering new situation [1, 1, 0] -> ?: \n",
      "Layer 0\n",
      "[ 0.29891013  0.41037941  0.16460788  0.33333159]\n",
      "Layer  1\n",
      "[ 0.52620089  0.66684022  0.28558447  0.40377279]\n",
      "Layer  2\n",
      "[ 0.46656931]\n",
      " \n",
      "[ 0.46656931]\n"
     ]
    }
   ],
   "source": [
    "# Initialise a multi layer neural network. (3 layers)\n",
    "# Each layer is represented by a tuple (nb_inputs, nb_neurons)\n",
    "neural_network = NeuralNetwork((3, 4), (4, 4), (4, 1))\n",
    "\n",
    "print(\"======Random starting Layer weights: ======\")\n",
    "\n",
    "for i in range(neural_network.nb_layer):\n",
    "    print \"Layer \", i\n",
    "    print neural_network.layer_weights[i]\n",
    "print \" \"\n",
    "            \n",
    "# The training set. We have 4 examples, each consisting of 3 input values\n",
    "# and 1 output value.\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "\n",
    "# Train the neural network using a training set.\n",
    "# Do it 10,000 times and make small adjustments each time.\n",
    "neural_network.train(training_set_inputs, training_set_outputs, 1) #10000\n",
    "\n",
    "print(\"====== New weights after training: ======\")\n",
    "for i in range(neural_network.nb_layer):\n",
    "    print \"Layer \", i+1\n",
    "    print neural_network.layer_weights[i]\n",
    "print \" \"\n",
    "\n",
    "# Test the neural network with a new situation.\n",
    "print (\"Considering new situation [1, 1, 0] -> ?: \")\n",
    "res = neural_network.think(array([1, 1, 0]))\n",
    "print res[-1]\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
